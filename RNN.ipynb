{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFhvNKmZ79GV"
      },
      "outputs": [],
      "source": [
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUgjEsBDQ_GK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwSN_-NTA8Sf"
      },
      "outputs": [],
      "source": [
        "# getting paths for HAUSA dataset\n",
        "train_set_path = '/content/gdrive/My Drive/Colab Notebooks/HAUSA/train.tsv'\n",
        "test_set_path = '/content/gdrive/My Drive/Colab Notebooks/HAUSA/test.tsv'\n",
        "validation_set_path = '/content/gdrive/My Drive/Colab Notebooks/HAUSA/dev.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIkJ8q6UeApy"
      },
      "outputs": [],
      "source": [
        "# getting paths for IGBO dataset\n",
        "train_set_path_1 = '/content/gdrive/My Drive/Colab Notebooks/IGBO/train.tsv'\n",
        "test_set_path_1 = '/content/gdrive/My Drive/Colab Notebooks/IGBO/test.tsv'\n",
        "validation_set_path_1 = '/content/gdrive/My Drive/Colab Notebooks/IGBO/dev.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtNL7CnIEI9Q"
      },
      "outputs": [],
      "source": [
        "# getting paths for NIGERIAN PIDGIN dataset\n",
        "train_set_path_2 = '/content/gdrive/My Drive/Colab Notebooks/PIDGIN/train.tsv'\n",
        "test_set_path_2 = '/content/gdrive/My Drive/Colab Notebooks/PIDGIN/test.tsv'\n",
        "validation_set_path_2 = '/content/gdrive/My Drive/Colab Notebooks/PIDGIN/dev.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk5nAkaH-Gyb"
      },
      "outputs": [],
      "source": [
        "# reading files for HAUSA dataset\n",
        "train_set = pd.read_csv(train_set_path, sep='\\t')\n",
        "test_set = pd.read_csv(test_set_path, sep='\\t')\n",
        "validation_set = pd.read_csv(validation_set_path, sep='\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZygm8n8jzSc"
      },
      "outputs": [],
      "source": [
        "# reading files for IGBO dataset\n",
        "train_set_1 = pd.read_csv(train_set_path_1, sep='\\t')\n",
        "test_set_1 = pd.read_csv(test_set_path_1, sep='\\t')\n",
        "validation_set_1 = pd.read_csv(validation_set_path_1, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOqtVNkZjzyd"
      },
      "outputs": [],
      "source": [
        "# reading files for  NIGERIAN PIDGIN dataset\n",
        "train_set_2 = pd.read_csv(train_set_path_2, sep='\\t')\n",
        "test_set_2 = pd.read_csv(test_set_path_2, sep='\\t')\n",
        "validation_set_2 = pd.read_csv(validation_set_path_2, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjiikRqWAEzx"
      },
      "outputs": [],
      "source": [
        "# preparing data for pre-processing\n",
        "train_set_data = np.array(train_set[\"tweet\"])\n",
        "train_labels = np.array(train_set[\"label\"])\n",
        "test_set_data = np.array(test_set[\"tweet\"])\n",
        "test_labels = np.array(test_set[\"label\"])\n",
        "validation_set_data = np.array(validation_set[\"tweet\"])\n",
        "val_labels = np.array(validation_set[\"label\"])\n",
        "\n",
        "train_set_data_1 = np.array(train_set_1[\"tweet\"])\n",
        "print(train_set_data_1)\n",
        "train_labels_1 = np.array(train_set_1[\"label\"])\n",
        "test_set_data_1 = np.array(test_set_1[\"tweet\"])\n",
        "test_labels_1 = np.array(test_set_1[\"label\"])\n",
        "validation_set_data_1 = np.array(validation_set_1[\"tweet\"])\n",
        "val_labels_1 = np.array(validation_set_1[\"label\"])\n",
        "\n",
        "train_set_data_2 = np.array(train_set_2[\"tweet\"])\n",
        "train_labels_2 = np.array(train_set_2[\"label\"])\n",
        "test_set_data_2 = np.array(test_set_2[\"tweet\"])\n",
        "test_labels_2 = np.array(test_set_2[\"label\"])\n",
        "validation_set_data_2 = np.array(validation_set_2[\"tweet\"])\n",
        "val_labels_2 = np.array(validation_set_2[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMOO2ewXLlHP"
      },
      "outputs": [],
      "source": [
        "# removing emojis from sentences\n",
        "def remove_emojis(sentence):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  \n",
        "        u\"\\U0001F300-\\U0001F5FF\"  \n",
        "        u\"\\U0001F680-\\U0001F6FF\"  \n",
        "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
        "        u\"\\U00002500-\\U00002BEF\"  \n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  \n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykec0F5YZFR4"
      },
      "outputs": [],
      "source": [
        "def data_clean_up(dataset):  \n",
        "  new_dataset = []\n",
        "  # removing emojis, twitter ids, URLs, punctuations, trailing spaces and digits\n",
        "  # from dataset\n",
        "  for line in dataset:\n",
        "    line_1 = \"\"\n",
        "    line_2 = line.split(\" \")\n",
        "    for word in line_2:\n",
        "      if len(word) != 0 and word[0] != \"@\" and (\"http\" not in word) and word[0] != \"#\" and any(c.isalpha() for c in word):\n",
        "        line_1 = line_1 + word + \" \"\n",
        "    new_line_1 = line_1.translate(str.maketrans('', '', string.digits))\n",
        "    new_line_2 = new_line_1.translate(str.maketrans('', '', string.punctuation))\n",
        "    new_line_3 = remove_emojis(new_line_2)\n",
        "    new_line_4 = new_line_3.lower()\n",
        "    new_dataset.append(new_line_4.rstrip())\n",
        "  return new_dataset\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EHOcWm8wbus"
      },
      "outputs": [],
      "source": [
        "train_data = data_clean_up(train_set_data)\n",
        "test_data = data_clean_up(test_set_data)\n",
        "val_data = data_clean_up(validation_set_data)\n",
        "\n",
        "\n",
        "train_data_1 = data_clean_up(train_set_data_1)\n",
        "test_data_1 = data_clean_up(test_set_data_1)\n",
        "val_data_1= data_clean_up(validation_set_data_1)\n",
        "\n",
        "train_data_2 = data_clean_up(train_set_data_2)\n",
        "test_data_2 = data_clean_up(test_set_data_2)\n",
        "val_data_2 = data_clean_up(validation_set_data_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIYlR5GlIAJD"
      },
      "outputs": [],
      "source": [
        "def get_vocab(dataset):\n",
        "  all_text = ' '.join(dataset)\n",
        "  words = all_text.split()\n",
        "  count_words = Counter(words)\n",
        "  total_words = len(words)\n",
        "  sorted_words = count_words.most_common(total_words)\n",
        "  vocab = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqfUPKBaujNN"
      },
      "outputs": [],
      "source": [
        "def encode(dataset, vocab):\n",
        "  words_int = []\n",
        "  for tweet in dataset:\n",
        "    r = [vocab[w] for w in tweet.split()]\n",
        "    words_int.append(r)\n",
        "  return words_int  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZKTprw-2YBY"
      },
      "outputs": [],
      "source": [
        "def encode_labels(labels):\n",
        "  encoded_labels = []\n",
        "  for label in labels:\n",
        "      if label == 'positive':\n",
        "          encoded_labels.append(1)\n",
        "      elif label == 'negative':\n",
        "          encoded_labels.append(0)\n",
        "      else:\n",
        "          encoded_labels.append(2)    \n",
        "\n",
        "  encoded_labels = np.array(encoded_labels)\n",
        "  return encoded_labels\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c60wdeU9vjXn"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_vocab = get_vocab(train_data)\n",
        "test_vocab = get_vocab(test_data)\n",
        "val_vocab =  get_vocab(val_data)\n",
        "\n",
        "train_vocab_1 = get_vocab(train_data_1)\n",
        "test_vocab_1 = get_vocab(test_data_1)\n",
        "val_vocab_1 =  get_vocab(val_data_1)\n",
        "\n",
        "train_vocab_2 = get_vocab(train_data_2)\n",
        "test_vocab_2 = get_vocab(test_data_2)\n",
        "val_vocab_2 =  get_vocab(val_data_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7So2YPJzDqm"
      },
      "outputs": [],
      "source": [
        "train_encoded = encode(train_data,train_vocab)\n",
        "test_encoded = encode(test_data,test_vocab)\n",
        "val_encoded = encode(val_data,val_vocab)\n",
        "\n",
        "train_encoded_1 = encode(train_data_1,train_vocab_1)\n",
        "test_encoded_1 = encode(test_data_1,test_vocab_1)\n",
        "val_encoded_1 = encode(val_data_1,val_vocab_1)\n",
        "\n",
        "\n",
        "train_encoded_2 = encode(train_data_2,train_vocab_2)\n",
        "test_encoded_2 = encode(test_data_2,test_vocab_2)\n",
        "val_encoded_2 = encode(val_data_2,val_vocab_2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73L0p_GY-cWV"
      },
      "outputs": [],
      "source": [
        "train_encoded_labels = encode_labels(train_labels)\n",
        "test_encoded_labels =  encode_labels(test_labels)\n",
        "val_encoded_labels = encode_labels(val_labels)\n",
        "\n",
        "train_encoded_labels_1 = encode_labels(train_labels_1)\n",
        "test_encoded_labels_1 =  encode_labels(test_labels_1)\n",
        "val_encoded_labels_1 = encode_labels(val_labels_1)\n",
        "\n",
        "train_encoded_labels_2= encode_labels(train_labels_2)\n",
        "test_encoded_labels_2 =  encode_labels(test_labels_2)\n",
        "val_encoded_labels_2 = encode_labels(val_labels_2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGKnI1ul2EVT"
      },
      "outputs": [],
      "source": [
        "seq_len = max([len(s.split()) for s in train_data])\n",
        "train_padded = pad_sequences(train_encoded , maxlen=seq_len, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_encoded , maxlen=seq_len, padding='post', truncating='post')\n",
        "val_padded = pad_sequences(val_encoded , maxlen=seq_len, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "seq_len_1 = max([len(s.split()) for s in train_data_1])\n",
        "train_padded_1 = pad_sequences(train_encoded_1 , maxlen=seq_len_1, padding='post', truncating='post')\n",
        "test_padded_1 = pad_sequences(test_encoded_1 , maxlen=seq_len_1, padding='post', truncating='post')\n",
        "val_padded_1 = pad_sequences(val_encoded_1 , maxlen=seq_len_1, padding='post', truncating='post')\n",
        "\n",
        "seq_len_2 = max([len(s.split()) for s in train_data_2])\n",
        "train_padded_2 = pad_sequences(train_encoded_2 , maxlen=seq_len_2, padding='post', truncating='post')\n",
        "test_padded_2 = pad_sequences(test_encoded_2 , maxlen=seq_len_2, padding='post', truncating='post')\n",
        "val_padded_2 = pad_sequences(val_encoded_2 , maxlen=seq_len_2, padding='post', truncating='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9NJg0mrtD4C"
      },
      "outputs": [],
      "source": [
        "def dataloaders(train_padded, train_encoded_labels, test_padded, test_encoded_labels, val_padded, val_encoded_labels):\n",
        "\n",
        "        # Load the data and labels into PyTorch tensors\n",
        "        train_data_ = torch.Tensor(train_padded)\n",
        "        train_labels = torch.Tensor(train_encoded_labels)\n",
        "        test_data_= torch.Tensor(test_padded)\n",
        "        test_labels = torch.Tensor(test_encoded_labels)\n",
        "        val_data_ = torch.Tensor(val_padded)\n",
        "        val_labels = torch.Tensor(val_encoded_labels)\n",
        "\n",
        "        # Create a TensorDataset for each dataset\n",
        "        train_dataset = TensorDataset(train_data_, train_labels)\n",
        "        test_dataset = TensorDataset(test_data_, test_labels)\n",
        "        val_dataset = TensorDataset(val_data_, val_labels)\n",
        "\n",
        "        # Create a DataLoader for each dataset\n",
        "        batch_size = 32\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "        return train_loader, test_loader, val_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fBk2uj1JSQ"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader, val_loader = dataloaders(train_padded, train_encoded_labels, test_padded, test_encoded_labels, val_padded, val_encoded_labels)\n",
        "train_loader_1, test_loader_1, val_loader_1 = dataloaders(train_padded_1, train_encoded_labels_1, test_padded_1, test_encoded_labels_1, val_padded_1, val_encoded_labels_1)\n",
        "train_loader_2, test_loader_2, val_loader_2 = dataloaders(train_padded_2, train_encoded_labels_2, test_padded_2, test_encoded_labels_2, val_padded_2, val_encoded_labels_2)                                                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN62Zh0J5xff"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, drop_prob=0.5):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        #input2hidden\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        #input2output\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob) \n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPlFWFNAVjG8"
      },
      "outputs": [],
      "source": [
        "def get_loss(input_size, hidden_size,  output_size, train_loader, val_loader):\n",
        "\n",
        "    rnn = RNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size,  drop_prob=0.5)\n",
        "\n",
        "    # Define your loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(rnn.parameters(), lr=0.001,)\n",
        "\n",
        "    # Define your training loop\n",
        "    num_epochs =50\n",
        "    \n",
        "    all_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        rnn.train()\n",
        "        train_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            hidden = rnn.initHidden()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for j in range(inputs.shape[0]):\n",
        "                output, hidden = rnn(inputs[j].view(1, -1), hidden)\n",
        "\n",
        "            loss = criterion(output, labels[j].view(1).long())\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        all_losses.append(train_loss)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "                   \n",
        "            for i, data in enumerate(val_loader, 0):\n",
        "                inputs, labels = data\n",
        "                hidden = rnn.initHidden()\n",
        "\n",
        "                for j in range(inputs.shape[0]):\n",
        "                    output, hidden = rnn(inputs[j].view(1, -1), hidden)\n",
        "\n",
        "                loss = criterion(output, labels[j].view(1).long())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        # Add the validation loss to the list of validation losses\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    return val_losses, all_losses, rnn\n",
        "        \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtM0uZoF3GTH"
      },
      "outputs": [],
      "source": [
        "val_losses, train_losses, model = get_loss(seq_len, seq_len, 3, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_losses_1, train_losses_1, model_1 = get_loss(seq_len_1, seq_len_1, 2, train_loader_1, val_loader_1)"
      ],
      "metadata": {
        "id": "-D963NKhOO_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_losses_2, train_losses_2, model_2 = get_loss(seq_len_2, seq_len_2, 2, train_loader_2, val_loader_2)"
      ],
      "metadata": {
        "id": "ItL8fnr0OPay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6RgIOlVcgsl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a list of epoch numbers\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "# plot the training and validation loss curves\n",
        "plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_losses, 'b', label='Validation loss')\n",
        "\n",
        "# add axis labels and legend\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgBn50qoEZ-Q"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader):\n",
        "\n",
        "    #Evaluate the model on the test set\n",
        "  \n",
        "    model.eval()\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_tn = 0\n",
        "    total_fn = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            labels = labels.numpy()\n",
        "            hidden = model.initHidden()\n",
        "            for i in range(inputs.shape[0]):\n",
        "                output, hidden = model(inputs[i].view(1, -1), hidden)\n",
        "\n",
        "            # Convert output probabilities to predicted class (0 or 1)\n",
        "            predicted = torch.argmax(output).item()\n",
        "\n",
        "            # Calculate the true positive, false positive, true negative, false negative\n",
        "            if predicted == 1 and round(labels[i]) == 1:\n",
        "                total_tp += 1\n",
        "            elif predicted == 1 and round(labels[i]) == 0:\n",
        "                total_fp += 1\n",
        "            elif predicted == 0 and round(labels[i]) == 0:\n",
        "                total_tn += 1\n",
        "            elif predicted == 0 and round(labels[i]) == 1:\n",
        "                total_fn += 1\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = total_tp / (total_tp + total_fp)\n",
        "    recall = total_tp / (total_tp + total_fn)\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return precision, recall, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1_score = evaluate(model, test_loader)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")"
      ],
      "metadata": {
        "id": "z66--pKJArGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1_score = evaluate(model_1, test_loader_1)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")"
      ],
      "metadata": {
        "id": "rYoObmXrO6Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1_score = evaluate(model_2, test_loader_2)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")"
      ],
      "metadata": {
        "id": "Gamcl4MsO6zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for inputs, labels in test_loader:\n",
        "#    labels = labels.numpy()\n",
        "#    for i in range(inputs.shape[0]):\n",
        "#      output, hidden = model(inputs[i].view(1, -1), hidden)\n",
        "#    print(output)"
      ],
      "metadata": {
        "id": "400iwM4mtbf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTZsbAwwoPD9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}